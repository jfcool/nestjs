# Ollama RAG System Setup Guide

## Overview

The documents module has been successfully migrated to use **Ollama** for local embedding generation, providing a free, high-quality, and privacy-focused RAG (Retrieval-Augmented Generation) system.

## Configuration

### Current Setup
- **Embedding Provider**: Ollama (local)
- **Model**: `nomic-embed-text`
- **Dimensions**: 768
- **Vector Database**: PostgreSQL with pgvector extension
- **Index Type**: HNSW with cosine similarity

### Environment Variables
```env
# Embedding Service Configuration - Using Ollama (Free, High Quality)
EMBEDDING_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSIONS=768
```

## Prerequisites

### 1. Install Ollama
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/
```

### 2. Start Ollama Service
```bash
ollama serve
```

### 3. Install Embedding Model
```bash
ollama pull nomic-embed-text
```

### 4. Verify Installation
```bash
ollama list
# Should show nomic-embed-text in the list
```

## Database Setup

The database has been configured with:
- **pgvector extension** enabled
- **768-dimensional vector column** for embeddings
- **HNSW index** for fast similarity search
- **Cosine similarity** for semantic matching

### Schema
```sql
-- Chunks table with embedding column
ALTER TABLE chunks ADD COLUMN embedding vector(768);

-- HNSW index for fast similarity search
CREATE INDEX idx_chunks_embedding_cosine 
ON chunks USING hnsw (embedding vector_cosine_ops) 
WITH (m = 16, ef_construction = 64);
```

## Testing

### 1. Database Setup Test
```bash
cd apps/api
node clear-and-setup-ollama.js
```

### 2. Complete RAG System Test
```bash
cd apps/api
node test-rag-system-ollama.js
```

## Features

### âœ… Completed
- [x] Ollama integration in EmbeddingService
- [x] 768-dimensional vector support
- [x] Database schema migration
- [x] HNSW indexing for performance
- [x] Comprehensive test suite
- [x] Document indexing with embeddings
- [x] Similarity search functionality
- [x] Clean data migration (old entries cleared)

### ğŸ”§ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documents     â”‚    â”‚  Ollama Local    â”‚    â”‚   PostgreSQL    â”‚
â”‚   Module        â”‚â”€â”€â”€â–¶â”‚  Embedding       â”‚â”€â”€â”€â–¶â”‚   + pgvector    â”‚
â”‚                 â”‚    â”‚  Generation      â”‚    â”‚   + HNSW Index  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                              â”‚
        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  RAG Search      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  & Retrieval     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance

### Embedding Generation
- **Model**: nomic-embed-text (768 dimensions)
- **Speed**: ~100-500ms per embedding (local)
- **Quality**: High semantic understanding
- **Cost**: Free (runs locally)

### Search Performance
- **Index**: HNSW with cosine similarity
- **Search Speed**: Sub-millisecond for similarity queries
- **Scalability**: Handles millions of vectors efficiently

## Usage in Code

### EmbeddingService
```typescript
// Automatically uses Ollama when EMBEDDING_PROVIDER=ollama
const embedding = await embeddingService.generateEmbedding(text);
const embeddings = await embeddingService.generateEmbeddings(texts);
```

### Document Indexing
```typescript
// Documents are automatically chunked and embedded
await documentIndexingService.indexDocument(document);
```

### Similarity Search
```typescript
// Search for relevant chunks
const results = await documentRetrievalService.searchSimilarChunks(query, limit);
```

## Advantages of Ollama

### ğŸš€ Benefits
- **Free**: No API costs or usage limits
- **Private**: All processing happens locally
- **Fast**: No network latency for embedding generation
- **Reliable**: No external service dependencies
- **High Quality**: nomic-embed-text provides excellent embeddings
- **Offline**: Works without internet connection

### ğŸ“Š Comparison

| Provider | Cost | Privacy | Speed | Quality | Offline |
|----------|------|---------|-------|---------|---------|
| Ollama   | Free | 100%    | Fast  | High    | âœ…      |
| OpenAI   | Paid | Low     | Fast  | High    | âŒ      |
| Anthropic| Paid | Low     | Medium| Medium  | âŒ      |

## Troubleshooting

### Common Issues

1. **Ollama not running**
   ```bash
   # Start Ollama service
   ollama serve
   ```

2. **Model not installed**
   ```bash
   # Install the embedding model
   ollama pull nomic-embed-text
   ```

3. **Connection refused**
   - Check if Ollama is running on port 11434
   - Verify OLLAMA_URL in .env file

4. **Dimension mismatch**
   - Ensure EMBEDDING_DIMENSIONS=768
   - Run database setup script to recreate vector column

### Logs and Debugging
```bash
# Check Ollama logs
ollama logs

# Test embedding generation
curl http://localhost:11434/api/embeddings \
  -d '{"model": "nomic-embed-text", "prompt": "test"}'
```

## Migration Notes

### What Changed
- âœ… Cleared all existing documents and chunks
- âœ… Updated vector column from 1536 to 768 dimensions
- âœ… Recreated HNSW index for new dimensions
- âœ… Configured EmbeddingService for Ollama
- âœ… Verified end-to-end functionality

### Rollback (if needed)
To rollback to OpenAI embeddings:
1. Update .env: `EMBEDDING_PROVIDER=openai`
2. Set `EMBEDDING_DIMENSIONS=1536`
3. Run database migration script
4. Add OpenAI API key

## Next Steps

1. **Production Deployment**: Ensure Ollama is installed on production servers
2. **Monitoring**: Add metrics for embedding generation performance
3. **Scaling**: Consider multiple Ollama instances for high load
4. **Model Updates**: Monitor for newer embedding models from Ollama

## Support

For issues or questions:
1. Check Ollama documentation: https://ollama.ai/
2. Review test scripts in `apps/api/test-*.js`
3. Check application logs for embedding service errors
4. Verify database schema and indexes

---

**Status**: âœ… **READY FOR PRODUCTION**

The RAG system is now fully operational with Ollama embeddings, providing free, private, and high-quality document search capabilities.
